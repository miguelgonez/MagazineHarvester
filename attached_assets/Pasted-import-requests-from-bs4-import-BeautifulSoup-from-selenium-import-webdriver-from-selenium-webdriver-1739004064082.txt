import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re

class JournalScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        }
        # Configurar opciones de Chrome
        self.chrome_options = webdriver.ChromeOptions()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')

    def get_articles_from_url(self, url):
        try:
            if not url.startswith('http'):
                url = f'https://{url}'

            st.write(f"Accediendo a: {url}")
            
            # Iniciar el navegador
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.get(url)
            
            # Esperar a que la página cargue completamente
            time.sleep(5)  # Dar tiempo para que el JavaScript se ejecute
            
            # Esperar a que aparezcan los elementos de los artículos
            wait = WebDriverWait(driver, 10)
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'article-item')))
            
            articles = []
            # Encontrar todos los artículos
            article_elements = driver.find_elements(By.CLASS_NAME, 'article-item')
            st.write(f"Artículos encontrados: {len(article_elements)}")

            for article in article_elements:
                try:
                    # Título
                    title = article.find_element(By.CSS_SELECTOR, 'h2 a').text.strip()
                    
                    # DOI
                    try:
                        doi = article.find_element(By.XPATH, ".//div[contains(text(), 'DOI:')]").text.replace('DOI:', '').strip()
                    except:
                        doi = None
                    
                    # PDF URL
                    try:
                        pdf_link = article.find_element(By.XPATH, ".//a[contains(text(), 'PDF')]")
                        pdf_url = pdf_link.get_attribute('href')
                    except:
                        pdf_url = None
                    
                    # Extraer volumen e issue de la URL
                    url_parts = url.split('/')[-1].split('-')
                    volume = int(url_parts[0].replace('articles/', ''))
                    issue = int(url_parts[1]) if len(url_parts) > 1 else 1

                    if title and pdf_url:
                        article_info = {
                            'title': title,
                            'doi': doi,
                            'pdf_url': pdf_url,
                            'volume': volume,
                            'issue': issue
                        }
                        articles.append(article_info)
                        st.write(f"Artículo encontrado: {title}")

                except Exception as e:
                    st.write(f"Error procesando artículo individual: {str(e)}")
                    continue

            driver.quit()
            st.write(f"Total artículos encontrados: {len(articles)}")
            return articles

        except Exception as e:
            st.error(f"Error en get_articles_from_url: {str(e)}")
            if 'driver' in locals():
                driver.quit()
            raise

    def download_pdf(self, pdf_url, filepath):
        try:
            session = requests.Session()
            response = session.get(pdf_url, headers=self.headers, stream=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            return True
        except Exception as e:
            st.error(f"Error descargando PDF: {str(e)}")
            return False